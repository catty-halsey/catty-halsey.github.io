---
layout: post
title: "Hypothesis test"
author: "Ziyan Li"
categories: journal
tags: [documentation, sample]
---

The hypothesis test allows us to test whether our sample (collected data) has certain properties. For example, we are interested in the mean, variance and other parameter of interest of one sample or two samples. In practice, we construct our test statistic to approximate the parameter of our interests. Now, the hypothesis test is provide us a tool to test whether our test statistic measure closely to our true parameter of interest or deviate from it. And if it close to the true parameter of interest, in what probability we can believe our results.

To construct a hypothesis, we first decide the null hypothesis $H_0$ against the alternative hypothesis $H_1$. Then, we need to design the test for null hypothesis.  There are two types of test, namely, the randomized test and non-randomized test. The randomized test maps $\mathscr{X}$ to the interval $[0,1]$ and the non-randomized test maps $\mathscr{X}$ to $\\{0,1\\}$. We define the significant level $\alpha$ as $\sup_{\theta \in \Theta_0} E_{\theta}[\phi(X)] \leq \alpha$. In hypothesis testing, we aim to evaluate a null hypothesis $H_0$ against an alternative hypothesis $H_1$ while controlling the probability of incorrectly rejecting $H_0$, known as the Type I error rate $\alpha$. To achieve this, we design our test function $\phi(X)$ to map observed data $X$ to $[0, 1]$, where $\phi(X) = 1$ indicates rejection of $H_0$. When the observed data is extreme or inconsistent with $H_0$, we want to classify it in the rejection region, thereby mapping these extreme values to $1$. This approach helps us control the proportion of extreme observations leading to rejection, ensuring that $E_{H_0}[\phi(X)] \leq \alpha$. This principle applies to both randomized and non-randomized tests, where the objective is to keep the likelihood of false rejections within our chosen significance level. 

Let we clarify some motivation of randomized test: When $0 < \phi(X) < 1$, it indicates that the decision to reject $H_0$ is not absoluteâ€”it is probabilistic. Specifically, if $\phi(X)$ returns a value $p$, we reject $H_0$ with probability $p$. The rationale is that, in the gray area, we recognize that the evidence against $H_0$ is not strong enough to guarantee rejection, but is sufficient to warrant a controlled chance of rejection. The Neyman-Pearson test is a classical randomized test.In the Neyman-Pearson test, when the likelihood ratio $\Lambda(x)$ equals some critical constant $k$, it indicates that the observed data is neither strongly supporting the null hypothesis $H_0$ nor the alternative hypothesis $H_1$. In this situation, you are at the threshold of making a decision.
